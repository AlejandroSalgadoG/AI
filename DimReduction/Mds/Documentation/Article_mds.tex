\documentclass[12pt,journal]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\title{A theorical-practical approach to manifold learning}
\author{A. Salgado - O. Luc√≠a Quintero M.}
\maketitle

\begin{abstract}
    This work presents an theorical approximation to the problem and provide an
    algorithmic solution easy and understandable. The approach to the problem is
    reconsider by the construction of the matrix $D^x$ with KNN. Creating the
    possibility to improve the results using other ways to calculate the distance
    matrix
\end{abstract}

\section{Introduction}

\section{Theorical framework}

    \subsection{Points from distances}

    The set up of the problem is a matrix of euclidean distances $D$, where each
    entry is the square of the distance between two points in a $d$ dimensional
    space.

    \[
        D =
        \begin{bmatrix}
            d_{11}^2 & \cdots & d_{1n}^2\\
            \vdots   & \ddots & \vdots  \\
            d_{n1}^2 & \ddots & d_{nn}^2\\
        \end{bmatrix}_{n x n}
    \]

    where

    \begin{equation*}
        d_{ij}^2 = (x_{i1}-x_{j1})^2 - (x_{i2}-x_{j2})^2 \cdots (x_{id}-x_{jd})^2
    \end{equation*}

    The unknown of the problem is a matrix $X$ that cotains the position of the
    $n$ points in the $d$ dimensional space.

    \[
        X =
        \begin{bmatrix}
            x_{11} & \cdots & x_{1n}\\
            \vdots & \ddots & \vdots\\
            x_{d1} & \ddots & x_{dn}\\
        \end{bmatrix}_{d x n}
    \]

    But there is a restriction to make sure that the matrix is centered at 0.

    \begin{equation*} 
        \sum_{i=1}^n x_{is} = 0 \quad with \quad s = 1, \cdots, d
    \end{equation*} 

    \subsection{Solution}

    First rewrite each entry of matrix $D$ in a vector form

    \begin{equation*}
        d_{ij}^2 = (x_i-x_j)^t (x_i-x_j)
    \end{equation*}

    Then expand distance $d_{ij}^2$

    \begin{equation*}
        \begin{aligned}
            (x_i-x_j)^t (x_i-x_j) &= (x_i^t-x_j^t) (x_i-x_j)\\
                                  &= x_i^t x_i - x_i^t x_j - x_j^t x_i + x_j^t x_j\\
                         d_{ij}^2 &= x_i^t x_i - 2 x_i^t x_j + x_j^t x_j\\
        \end{aligned}
    \end{equation*}

    The goal is to express all the dot products of $x$ vectors in terms of the
    $d_{ij}^2$ distances. Consider the following expression

    \begin{equation*}
        \begin{aligned}
            \frac{1}{n} \sum_{i=1}^n d_{ij}^2 &= \frac{1}{n} \sum_{i=1}^n [x_i^t x_i - 2 x_i^t x_j + x_j^t x_j]\\
            &=  \frac{1}{n} \sum_{i=1}^n x_i^t x_i - \frac{2}{n} \sum_{i=1}^n x_i^t x_j + \frac{1}{n} \sum_{i=1}^n x_j^t x_j\\
            &=  \frac{1}{n} \sum_{i=1}^n x_i^t x_i - \frac{2}{n} \sum_{i=1}^n x_i^t x_j + \frac{n}{n} x_j^t x_j \\
        \end{aligned}
    \end{equation*}

    Remember that

    \begin{equation*}
        \begin{aligned}
              \sum_{i=1}^n x_{is} &= 0 \quad with \quad s = 1, \cdots, d\\
            \sum_{i=1}^n x_i^t x_j &= \sum_{i=1}^n \sum_{r=1}^d x_{ir} x_{jr}\\
                                   &= \sum_{r=1}^d \sum_{i=1}^n x_{ir} x_{jr}\\
                                   &= \sum_{r=1}^d x_{jr} \sum_{i=1}^n x_{ir}\\
                                   &= \sum_{r=1}^d x_{jr} 0\\
                                   &= 0\\
        \end{aligned}
    \end{equation*}

    Hence

    \begin{equation*}
        \begin{aligned}
            \frac{1}{n} \sum_{i=1}^n d_{ij}^2
            &= 
            \frac{1}{n} \sum_{i=1}^n x_i^t x_i - \frac{2(0)}{n} + x_j^t x_j\\
            &= 
            \frac{1}{n} \sum_{i=1}^n x_i^t x_i + x_j^t x_j\\
            x_j^t x_j &= \frac{1}{n} \sum_{i=1}^n d_{ij}^2 - \frac{1}{n} \sum_{i=1}^n x_i^t x_i
        \end{aligned}
    \end{equation*}

    Now in the other hand, the sum over $j$ would be

    \begin{equation*}
        \begin{aligned}
            \frac{1}{n} \sum_{j=1}^n d_{ij}^2
            &=
            \frac{1}{n} \sum_{j=1}^n [x_i^t x_i - 2 x_i^t x_j + x_j^t x_j]\\
            &= 
            x_i^t x_i + \frac{1}{n} \sum_{j=1}^n x_j^t x_j\\
            x_i^t x_i &= \frac{1}{n} \sum_{j=1}^n d_{ij}^2 - \frac{1}{n} \sum_{j=1}^n x_j^t x_j\\
        \end{aligned}
    \end{equation*}

    Finally, the first approximation to the problem can be constructed

    \begin{equation*}
        \begin{aligned}
            d_{ij}^2 &= x_i^t x_i - 2 x_i^t x_j + x_j^t x_j\\
            2 x_i^t x_j &= x_i^t x_i - d_{ij}^2 + x_j^t x_j\\
            x_i^t x_j &= \frac{1}{2} (x_i^t x_i - d_{ij}^2 + x_j^t x_j)\\
            \text{where}\\
            x_i^t x_i &= \frac{1}{n} \sum_{j=1}^n d_{ij}^2 - \frac{1}{n} \sum_{j=1}^n x_j^t x_j\\
            x_j^t x_j &= \frac{1}{n} \sum_{i=1}^n d_{ij}^2 - \frac{1}{n} \sum_{i=1}^n x_i^t x_i
        \end{aligned}
    \end{equation*}

    Notice that

    \begin{equation*}
        \sum_{i=1}^n x_i^t x_i = \sum_{j=1}^n x_j^t x_j
    \end{equation*}

    So

    \begin{equation*}
        \begin{aligned}
            x_i^t x_j&=
            \frac{1}{2} \left[ \frac{1}{n} \sum_{j=1}^n d_{ij}^2 - d_{ij}^2 + \frac{1}{n} \sum_{i=1}^n d_{ij}^2 - \frac{2}{n} \sum_{i=1}^n x_i^t x_i \right]\\
        \end{aligned}
    \end{equation*}

    This is almost expressed in terms of the distances, the only problem is the
    last term. To express that part consider the following expression

    \begin{equation*}
        \begin{aligned}
            \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n d_{ij}^2
            &= 
            \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n [x_i^t x_i - 2 x_i^t x_j + x_j^t x_j]\\
            &= 
            \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n x_i^t x_i - \frac{2(0)}{n^2} +  \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^nx_j^t x_j\\
            &= 
            \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n x_i^t x_i +  \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^nx_j^t x_j\\
            &= 
            \frac{n}{n^2}\sum_{i=1}^n x_i^t x_i +  \frac{n}{n^2}\sum_{j=1}^n x_j^t x_j\\
            &= 
            \frac{1}{n}\sum_{i=1}^n x_i^t x_i +  \frac{1}{n}\sum_{j=1}^n x_j^t x_j\\
            &= 
            \frac{2}{n}\sum_{i=1}^n x_i^t x_i\\
        \end{aligned}
    \end{equation*}

    Now the expression can be completed
    \begin{equation*}
        \begin{aligned}
            x_i^t x_j
            &=
            \frac{1}{2} \left[ \frac{1}{n} \sum_{i=1}^n d_{ij}^2 - d_{ij}^2 + \frac{1}{n} \sum_{j=1}^n d_{ij}^2 - \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n d_{ij}^2 \right]\\
        \end{aligned}
    \end{equation*}

    This computation can be expressed in matrix form to simplify the formula.
    First define

    \[
        b_{ij} = x_i^t x_j
        \hspace{1cm}
        e =
        \begin{bmatrix}
            1\\
            \vdots\\
            1
        \end{bmatrix}_{nx1}
        \hspace{1cm}
        a_{ij} = -\frac{1}{2} d_{ij}^2
    \]

    Now a matrix $B$ can be defined as

    \begin{equation*}
        B = H A H \quad \text{where} \quad H = I - \frac{1}{n} e e^t
    \end{equation*}

    This formula gives a way to compute all the dot products between the points
    that makes up the matrix $X$. Because of this the matrix $B$ can also
    be expressed as

    \begin{equation*}
        B = X^tX
    \end{equation*}

    Now, as shown in section \ref{orthogonal}, $B$ can be decomposed using the
    spectral decomposition. And by doing that the problem can be solved as follows

    \begin{equation*}
        \begin{aligned}
            B &= V \Sigma V^t\\
              &= V \Sigma^{1/2} \Sigma^{1/2} V^t\\
              &= V \Sigma^{1/2} \Sigma^{1/2} V^t\\
              &= X^t X\\
            X & = \Sigma^{1/2} V^t
        \end{aligned}
    \end{equation*}

    Which gives a way to compute all the points of the matrix $X$.

    \section{Mds optimization problem}

    \begin{equation*}
        \begin{aligned}
            \underset{y}{\text{min}}  & \sum_{i=1}^n \sum_{j=1}^n ( d^{(x)}_{ij} - d^{(x)}_{ij} )^2 \\
            & d_{ij}^{(x)} = \lVert x_i - x_j \rVert\\
            & d_{ij}^{(y)} = \lVert y_i - y_j \rVert
        \end{aligned}
    \end{equation*}\\

    if $d_{ij}^{(x)}$ es euclidean, then $d_{ij}^{(x)} = x_i^t x_j$, and hence

    \begin{equation*}
        \underset{y}{\text{min}} \sum_{i=1}^n \sum_{j=1}^n ( x_i^t x_j - y_i^t y_j )^2 \\
    \end{equation*}

    or in matrix form

    \begin{equation*}
        \underset{Y}{\text{min}} \quad \lVert X^t X - Y^t Y \rVert^2 \\
    \end{equation*}

    As shown in section \ref{norm_trace}, the norm of a matrix can be transformed
    to a trace, so the problem can be redefined as

    \begin{equation*}
        \begin{aligned}
            A = X^tX &- Y^tY\\
            \underset{Y}{\text{min}} \quad \lVert X^t X - Y^t Y \rVert^2
            &=
            \underset{Y}{\text{min}} \quad \lVert A \rVert^2\\
            &=
            \underset{Y}{\text{min}} \quad Tr(A^tA) \\
        \end{aligned}
    \end{equation*}\\

    Now

    \begin{equation*}
        \begin{aligned}
            A^t &= (X^tX - Y^tY)^t\\
                &= (X^tX)^t - (Y^tY)^t\\
                &= X^tX - Y^tY\\
                &= A
        \end{aligned}
    \end{equation*}

    Which means that $A^tA = AA = A^2$, and hence

    \begin{equation*}
        \begin{aligned}
            \underset{Y}{\text{min}} \quad Tr(A^tA)
            &=
            \underset{Y}{\text{min}} \quad Tr(A^2)\\
            &=
            \underset{Y}{\text{min}} \quad Tr[(X^tX - Y^tY)^2]
        \end{aligned}
    \end{equation*}\\

    now as shown in section \ref{spectral_decomp}, $X^tX$ and $Y^tY$ can be
    expresed as

    \begin{equation*}
        X^tX = V\Lambda V^t \hspace{1cm} Y^tY = Q \hat{\Lambda} Q^t
    \end{equation*}

    Then

    \begin{equation*}
        \underset{Y}{\text{min}} \quad Tr[(X^tX - Y^tY)^2]
        =
        \underset{Q,\hat{\Lambda}}{\text{min}} \quad Tr[(V\Lambda V^t - Q \hat{\Lambda} Q^t)^2]
    \end{equation*}

    We know that $V^tV = I = VV^t$, then using the proof presented in section \ref{circular_trace}

    \begin{equation*}
        \begin{aligned}
            A =& V\Lambda V^t - Q \hat{\Lambda} Q^t\\
            Tr[(V^tAV)^2] &= Tr[V^tAVV^tAV]\\
                          &= Tr[V^tAIAV]\\
                          &= Tr[AIAVV^t]\\
                          &= Tr[AIAI]\\
                          &= Tr[AA]\\
                          &= Tr[A^2]\\
        \end{aligned}
    \end{equation*}\\

    Which implies that

    \begin{equation*}
        \begin{aligned}
            Tr[(V^tAV)^2] &= Tr[A^2]\\
            Tr[(V^t(V\Lambda V^t - Q \hat{\Lambda} Q^t)V)^2] &= Tr[(V\Lambda V^t - Q \hat{\Lambda} Q^t)^2]\\
            Tr[(V^tV\Lambda V^tV - V^tQ \hat{\Lambda} Q^tV)^2] &=\\
            Tr[(I\Lambda I - V^tQ \hat{\Lambda} Q^tV)^2] &=\\
            Tr[(\Lambda - V^tQ \hat{\Lambda} Q^tV)^2] &=\\
        \end{aligned}
    \end{equation*}

    Now

    \begin{equation*}
        \begin{aligned}
            G &= V^tQ\\
            Tr[(\Lambda - V^tQ \hat{\Lambda} Q^tV)^2] &= Tr[(\Lambda - G \hat{\Lambda} G^t)^2]\\
        \end{aligned}
    \end{equation*}\\

    Which means that the problem can be redefined one more time

    \begin{equation*}
        \begin{aligned}
        \underset{Q,\hat{\Lambda}}{\text{min}} \quad Tr[(V\Lambda V^t - Q \hat{\Lambda} Q^t)^2]
        &=
        \underset{G,\hat{\Lambda}}{\text{min}} \quad Tr[(\Lambda - G \hat{\Lambda} G^t)^2]
        \end{aligned}
    \end{equation*}\\

    Now

    \begin{equation*}
        \begin{aligned}
        Tr[(\Lambda - G \hat{\Lambda} G^t)^2]
        &=
        Tr[\Lambda^2 - 2 \Lambda G \hat{\Lambda} G^t + (G \hat{\Lambda} G^t)^2]\\
        &=
        Tr[\Lambda^2 - 2 \Lambda G \hat{\Lambda} G^t + G \hat{\Lambda} G^t G \hat{\Lambda} G^t]\\
        \end{aligned}
    \end{equation*}

    But (Knowing that $Q^tQ = I = QQ^t$)

    \begin{equation*}
        \begin{aligned}
            G^tG &= (V^tQ)^t V^tQ\\
                 &= (Q^tV) V^tQ\\
                 &= Q^tQ\\
                 &= I\\ \\
            GG^t &= V^tQ (V^tQ)^t\\
                 &= V^tQ (Q^tV)\\
                 &= V^tV\\
                 &= I\\ \\
        \end{aligned}
    \end{equation*}

    Hence

    \begin{equation*}
        \begin{aligned}
        Tr[(\Lambda - G \hat{\Lambda} G^t G)^2 ]
        &=
        Tr[\Lambda^2 - 2 \Lambda G \hat{\Lambda} G^t + G \hat{\Lambda} \hat{\Lambda} G^t]\\
        =
        Tr[&\Lambda^2 - 2 \Lambda G \hat{\Lambda} G^t + G \hat{\Lambda}^2 G^t]\\
        =
        Tr[&\Lambda^2]- 2 Tr[\Lambda G \hat{\Lambda} G^t] + Tr[G \hat{\Lambda}^2 G^t]\\
        =
        Tr[&\Lambda^2] - 2 Tr[\Lambda G \hat{\Lambda} G^t] + Tr[ \hat{\Lambda}^2 G^tG]\\
        =
        Tr[&\Lambda^2] - 2 Tr[\Lambda G \hat{\Lambda} G^t] + Tr[ \hat{\Lambda}^2]\\
        =
        Tr[&\Lambda^2] + Tr[ \hat{\Lambda}^2] - 2 Tr[\Lambda G \hat{\Lambda} G^t]\\
        \end{aligned}
    \end{equation*}

    \subsection{Dark part}

    We know that $\frac{\delta Tr[\Lambda G \hat{\Lambda} G^t]}{\delta G} = 2\Lambda G \hat{\Lambda}$\\\\
    We also know that $\frac{\delta Tr(\Lambda^2)}{\delta G} = 0 = \frac{\delta Tr(\hat{\Lambda}^2)}{\delta G}$\\\\
    Finally we know that the answer for a fixed $\hat{\Lambda}$ is $G=I$\\ \\

    Which implies that

    \begin{equation*}
        \begin{aligned}
        G = I = V^tQ\\
        V^tQ = I\\
        V(V^tQ) = VI\\
        Q = V
        \end{aligned}
    \end{equation*}

    Now, coming back to the original problem

    \begin{equation*}
        \begin{aligned}
            \underset{G,\hat{\Lambda}}{\text{min}} \quad Tr[(\Lambda - G \hat{\Lambda} G^t)^2]
            &=
            \underset{\hat{\Lambda}}{\text{min}} \quad Tr[(\Lambda - \hat{\Lambda})^2]
        \end{aligned}
    \end{equation*}

    So we can take the first $p$ eigenvalues of $\Lambda$ to define $\hat{\Lambda}$,
    and finally

    \begin{equation*}
        \begin{aligned}
            Y^tY &= Q \hat{\Lambda} Q^t\\
            &=
            V \hat{\Lambda} V^t\\
            &=
            V \hat{\Lambda}^{1/2} \hat{\Lambda}^{1/2} V^t\\
        \end{aligned}
    \end{equation*}

    which implies that $Y = \hat{\Lambda}^{1/2} V^t$

    \section{Miscellaneous proofs}

    \subsection{Proof of the relationship between the norm and the trace} \label{norm_trace}

    \begin{equation*}
        \begin{aligned}
            \lVert A \rVert^2 = \sum_{i=1}^n \sum_{j=1}^n A_{ij}^2
            \hspace{1cm}
            Tr(A) = \sum_{i=1}^n A_{ii}
        \end{aligned}
    \end{equation*}\\

    Concider a matrix B that is computed as $A^tA$

    \begin{equation*}
        B = A^tA
    \end{equation*}

    Then an element of B can be computed as

    \begin{equation*}
        B_{ij} = \sum_{k=1}^n A_{ik}^t A_{kj} = \sum_{k=1}^n A_{ki} A_{kj} = (A^tA)_{ij}
    \end{equation*}

    Now, the trace of B will be

    \begin{equation*}
        \begin{aligned}
            Tr(B) &= T(A^tA)\\
                  &= \sum_{i=1}^n (A^tA)_{ii}\\
                  &= \sum_{i=1}^n \sum_{j=1}^n A_{ij}^t A_{ji}\\
                  &= \sum_{i=1}^n \sum_{j=1}^n A_{ji} A_{ji}\\
                  &= \sum_{i=1}^n \sum_{j=1}^n A_{ji}^2 \\
                  &= \sum_{i=1}^n \sum_{j=1}^n A_{ij}^2 \\
                  &= \lVert A \rVert^2
        \end{aligned}
    \end{equation*}

    Hence

    \begin{equation*}
        \lVert A \rVert^2 = Tr(A^tA)
    \end{equation*}

    \subsection{Proof of the circular transformation of the trace} \label{circular_trace}

    Consider a matrix M which is the multiplication of matrices B and C

    \begin{equation*}
        M = BC
    \end{equation*}

    Then an element of M is computed as

    \begin{equation*}
        M_{ij} = \sum_{k=1}^n B_{ik} C_{kj} = BC_{ij}
    \end{equation*}

    Now, the trace of M will be

    \begin{equation*}
        \begin{aligned}
            Tr(M) &= Tr(BC)\\
                  &= \sum_{i=1}^n BC_{ii}\\
                  &= \sum_{i=1}^n \sum_{j=1}^n B_{ij} C_{ji}\\
                  &= \sum_{i=1}^n \sum_{j=1}^n C_{ij} B_{ji}\\
                  &= \sum_{i=1}^n CB_{ii}\\
                  &= Tr(CB)
        \end{aligned}
    \end{equation*}

    hence

    \begin{equation*}
        Tr(BC) = Tr(CB)
    \end{equation*}

    \subsection{Relationship between transpose and inverse of an othogonal matrix}
    \label{orthogonal}
    
    Consider an orthogonal matrix $Q$, 

    \[
        Q =
        \begin{bmatrix}
             |  &        &  | \\
            q_1 & \cdots & q_n\\
             |  &        &  | \\
        \end{bmatrix}
    \]

    Since is orthogonal the following expression holds

    \[
        q_i q_j = 
        \begin{cases} 
           0 & i \neq j \\
           \lVert q_i \rVert & i = j 
        \end{cases}
    \]

    And then

    \[
        Q^t Q =
        \begin{bmatrix}
             - q_1^t - \\
               \vdots  \\
             - q_n^t - \\
        \end{bmatrix}
        \begin{bmatrix}
             |  &        &  | \\
            q_1 & \cdots & q_n\\
             |  &        &  | \\
        \end{bmatrix}
    \]
    \[
        =
        \begin{bmatrix}
            q_1^t q_1 & \cdots & q_1^t q_n \\
              \vdots  & \ddots &  \vdots   \\
            q_n^t q_1 & \cdots & q_n^t q_n \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          \lVert q_1 \rVert &         0         & \cdots &         0    \\
                  0         & \lVert q_2 \rVert & \cdots &         0    \\
               \vdots       &       \vdots      & \ddots &       \vdots \\
                  0         &         0         & \cdots & \lVert q_n \rVert \\
        \end{bmatrix}
    \]

    Which implies that if all the vectors of $Q$ have a length of one ($Q$ is
    orthonormal), then

    \begin{equation*}
        Q^t Q = I
    \end{equation*}

    And if $Q$ is also sqare, then

    \begin{equation*}
        Q^t = Q^{-1}
    \end{equation*}

    \subsection{Spectral decomposition for symmetric matrices} \label{spectral_decomp}

    Consider a symmetric matrix $A$, then this matrix will have $n$ orthogonal
    eigenvalues. Then build a new matrix $S$ that will contain the eigenvalues of
    $A$ as columns.\\

    Now from the definition of eigenvalues and eigenvectors

    \begin{equation*}
        A \mu = \lambda \mu
    \end{equation*}

    If $\mu$ is replaced with $S$ the result would be

    \[
        A S = A 
        \begin{bmatrix}
              |   &        &   |  \\
            \mu_1 & \cdots & \mu_n\\
              |   &        &   |  \\
        \end{bmatrix}
        =
        \begin{bmatrix}
             & - x_1^t - &\\
             &  \vdots   &\\
             & - x_n^t - &\\
        \end{bmatrix}
        \begin{bmatrix}
              |   &        &   |  \\
            \mu_1 & \cdots & \mu_n\\
              |   &        &   |  \\
        \end{bmatrix}\\
    \]
    \[
        =
        \begin{bmatrix}
            x_1^t \mu_1 & \cdots & x_1^t \mu_n\\
               \vdots   & \ddots &   \vdots   \\
            x_n^t \mu_1 & \cdots & x_n^t \mu_n\\
        \end{bmatrix}
        =
        \begin{bmatrix}
                    |       &        &         |      \\
            \lambda_1 \mu_1 & \cdots & \lambda_n \mu_n\\
                    |       &        &         |      \\
        \end{bmatrix}\\
    \]
    \[
        =
        \begin{bmatrix}
               |  &        &   |  \\
            \mu_1 & \cdots & \mu_n\\
               |  &        &   |  \\
        \end{bmatrix}\\
        \begin{bmatrix}
            \lambda_1 &      0    & \cdots &   0 \\
                0     & \lambda_2 & \cdots &   0 \\
              \vdots  &   \vdots  & \ddots & \vdots \\
                0     &      0    & \cdots & \lambda_n \\
        \end{bmatrix}\\
    \]

    So, if a new matrix $\Lambda$ is define as a diagonal matrix that contains
    the eigenvalues of $A$, then the computation can be expressed as

    \begin{equation*}
        A S = S \Lambda
    \end{equation*}

    And, since $S$ is invertible

    \begin{equation*}
        A = S \Lambda S^{-1}
    \end{equation*}

    Finally, as shown in section \ref{orthogonal}, the inverse of an orthonormal
    matrix is its traspose, then A can be expressed as

    \begin{equation*}
        A = S \Lambda S^t
    \end{equation*}

\section{Algorithm implementation}

\section{Results}

\section{Conclusions}

\end{document}
